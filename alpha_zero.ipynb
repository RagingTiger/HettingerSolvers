{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579f7611-3303-432d-80b2-1a67ff35154c",
   "metadata": {},
   "source": [
    "*   [Overview](overview.ipynb)\n",
    "*   [Depth First and Breath First Search](puzzle.ipynb)\n",
    "*   [SAT Solvers](einstein.ipynb)\n",
    "*   [Pattern Recognition and Reinforcement Learning](rock_paper.ipynb)\n",
    "*   [SMT and Model Checkers](dining.ipynb)\n",
    "*   [AlphaZero](#) <--- (You Are Here)\n",
    "    *   [The Approach](#the-approach)\n",
    "    *   [Code You Can Run](#code-you-can-run)\n",
    "    *   [Overview of Monte Carlo Tree Search](#overview-of-monte-carlo-tree-search)\n",
    "    *   [The Algorithm](#the-algorithm)\n",
    "*   [The Future](philosophy.ipynb)\n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c8855-1287-46d9-9543-ec175a36730f",
   "metadata": {},
   "source": [
    "AlphaZero[¶](#alphazero \"Permalink to this headline\")\n",
    "=====================================================\n",
    "\n",
    "A _little_ bit harder problem:\n",
    "\n",
    "*   Learn to play complex games given _only_ the rules to the game\n",
    "*   Beat the best human players _and_ their best hand-written programs\n",
    "*   Learning time limit: Less than a day\n",
    "\n",
    "![_images/chess_shogi_go.png](_images/chess_shogi_go.png)\n",
    "\n",
    "Typical result:\n",
    "\n",
    "```\n",
    "Game 3 - White: AlphaZero Black: Stockfish\n",
    "\n",
    "1. d4 Nf6 2. c4 e6 3. Nf3 b6 4. g3 Bb7 5. Bg2 Bb4+ 6. Bd2 Be7 7. Nc3 O-O\n",
    "8. Qc2 Na6 9. a3 c5 10. d5 exd5 11. Ng5 Nc7 12. h4 h6 13. Nxd5 Ncxd5\n",
    "14. cxd5 d6 15. a4 Qd7 16. Bc3 Rfe8 17. O-O-O Bd8 18. e4 Ng4\n",
    "19. Bh3 hxg5 20. f3 f5 21. fxg4 fxg4 22. Bf1 gxh4 23. Bb5 Qf7\n",
    "24. gxh4 Bf6 25. Rhf1 Rf8 26. Bxf6 gxf6 27. Rf4 Qg7 28. Be2 Qh6\n",
    "29. Rdf1 g3 30. Qd3 Kh8 31. Qxg3 Rae8 32. Bd3 Bc8 33. Kb1 Rf7\n",
    "34. Qf2 Bd7 35. h5 Ref8 36. Bc2 Be8 37. Rf3 Re7 38. Rxf6 Qxf6\n",
    "39. Qxf6+ Rxf6 40. Rxf6 Kg7 41. Rxd6 Bxh5 42. Kc1 Re5 43. a5 bxa5\n",
    "44. Kd2 Be8 45. Ra6 Rh5 46. Bd3 a4 47. d6 Bf7 48. d7 Rh8 49. e5 1-0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aade9e3a-aa91-495e-97a2-dc954f8c0e89",
   "metadata": {},
   "source": [
    "The Approach[¶](#the-approach \"Permalink to this headline\")\n",
    "-----------------------------------------------------------\n",
    "\n",
    "1.  MCTS – Monte Carlo Tree Search\n",
    "2.  Reinforcment Learning\n",
    "3.  Deep convolutional neural networks\n",
    "\n",
    "The DeepMind team published their results five months ago in the December 2018 _Science_, “A general reinforcement learning algorithm that masters chess, shogi and Go through self-play”. See: [https://deepmind.com/documents/260/alphazero\\_preprint.pdf](https://deepmind.com/documents/260/alphazero_preprint.pdf)\n",
    "\n",
    "Their self-assessment:\n",
    "\n",
    "> “Our results demonstrate that a general-purpose reinforcement learning algorithm can learn, tabula rasa – without domain-specific human knowledge or data, as evidenced by the same algorithm succeeding in multiple domains – superhuman performance across multiple challenging games.”\n",
    "\n",
    "Assessment by Garry Kasparov:\n",
    "\n",
    "> “I can’t disguise my satisfaction that it plays with a very dynamic style, much like my own!”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad934d-cdbb-4db2-8d0d-2e56962b8a6a",
   "metadata": {},
   "source": [
    "Code You Can Run[¶](#code-you-can-run \"Permalink to this headline\")\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "The advancements from _AlphaZero_ were incorporated in incorporated _Leela Chess Zero_\n",
    "\n",
    "Leela Chess is free software. See [https://github.com/LeelaChessZero/lc0](https://github.com/LeelaChessZero/lc0)\n",
    "\n",
    "It is built using _Meson_ with Python 3 but running primary CUDA code on the GPU.\n",
    "\n",
    "See the [LCBlog blog](http://blog.lczero.org/2018/12/alphazero-paper-and-lc0-v0191.html) to get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e58507-0f43-40dd-948b-7641d1ef3c0d",
   "metadata": {},
   "source": [
    "Overview of Monte Carlo Tree Search[¶](#overview-of-monte-carlo-tree-search \"Permalink to this headline\")\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "![_images/MCTS_(English)_-_Updated_2017-11-19.svg](_images/MCTS_(English)_-_Updated_2017-11-19.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36e459-0cb5-4df9-948b-20448dab7bb4",
   "metadata": {},
   "source": [
    "The Algorithm[¶](#the-algorithm \"Permalink to this headline\")\n",
    "-------------------------------------------------------------\n",
    "\n",
    "Here is the outline as summarized in the DeepMind paper:\n",
    "\n",
    "> AlphaZero replaces the handcrafted knowledge and domain-specific augmentations used in traditional game-playing programs with deep neural networks, a general-purpose reinforcement learning algorithm, and a general-purpose tree search algorithm.\n",
    "> \n",
    "> Instead of a handcrafted evaluation function and move ordering heuristics, AlphaZero uses a deep neural network (p, v) = fθ(s) with parameters θ. This neural network fθ(s) takes the board position s as an input and outputs a vector of move probabilities p with components pa = Pr(a|s) for each action a, and a scalar value v estimating the expected outcome z of the game from position s, v ≈ E\\[z|s\\]. AlphaZero learns these move probabilities and value estimates entirely from self-play; these are then used to guide its search in future games.\n",
    "> \n",
    "> Instead of an alpha-beta search with domain-specific enhancements, AlphaZero uses a generalpurpose Monte Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root state sroot until a leaf state is reached. Each simulation proceeds by selecting in each state s a move a with low visit count (not previously frequently explored), high move probability and high value (averaged over the leaf states of simulations that selected a from s) according to the current neural network fθ. The search returns a vector π representing a probability distribution over moves, πa = Pr(a|sroot).\n",
    "> \n",
    "> The parameters θ of the deep neural network in AlphaZero are trained by reinforcement learning from self-play games, starting from randomly initialized parameters θ. Each game is played by running an MCTS search from the current position sroot = st at turn t, and then selecting a move, at ∼ πt , either proportionally (for exploration) or greedily (for exploitation) with respect to the visit counts at the root state. At the end of the game, the terminal position sT is scored according to the rules of the game to compute the game outcome z: −1 for a loss, 0 for a draw, and +1 for a win. The neural network parameters θ are updated to minimize the error between the predicted outcome vt and the game outcome z, and to maximize the similarity of the policy vector pt to the search probabilities πt. The updated parameters are used in subsequent games of self-play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840dd3d-d47b-4f1f-929f-3e6949932b3f",
   "metadata": {},
   "source": [
    "| [<<< Previous](dining.ipynb \"SMT and Model Checkers\") | [Next >>>](philosophy.ipynb \"The Future\") |\n",
    "|                     :-:                           |                    :-:                |   \n",
    "* * *\n",
    "\n",
    "© Copyright 2019, Raymond Hettinger|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
